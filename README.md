# Real-Time Hand Sign Recognition using CNN-LSTM

A deep learning-based real-time sign language recognition system that bridges the communication gap between the deaf and hearing communities using Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM) models.

⸻

 ## 📄 Abstract

This project implements a real-time hand sign recognition system using a hybrid CNN-LSTM architecture. The system is capable of capturing spatial and temporal features from hand gestures in video streams, allowing accurate recognition of dynamic signs. This helps facilitate smoother communication with individuals with hearing or speech impairments.

⸻

## 🧠 Technologies Used
	•	Python
	•	TensorFlow / PyTorch
	•	OpenCV
	•	MediaPipe (optional for hand landmarks)
	•	NumPy, Matplotlib, etc.

⸻

## 🧪 Model Architecture
	•	CNN: Extracts spatial features from image frames
	•	LSTM: Captures temporal dynamics in sequences of gestures
	•	Hybrid Architecture: CNN → Feature Vectors → LSTM → Sign Classification

⸻

## ⚙️ Features
	•	Real-time hand gesture detection using webcam
	•	CNN for static frame analysis
	•	LSTM for sequential temporal analysis
	•	Live prediction overlay with confidence score
	•	Lightweight and optimized model (suitable for mobile/embedded use)
 
⸻

## 📈 Results
	•	Accuracy Achieved: 90.86% on validation data
	•	Robust in varied lighting/background conditions
	•	Responsive real-time performance using optimized CNN design

⸻

## 📌 Future Scope
	•	Extend support for full ASL/BSL alphabets and phrases
	•	Deploy model to mobile/edge devices using TensorFlow Lite
	•	Integrate voice output for translation
	•	Extend to 3D hand gesture detection using depth sensors

⸻

## 👨‍💻 Contributors
	•	Pathakota Rahul Reddy
	•	Akram Ali
	•	Vikramaditya Mishra
	•	Marifat Rashid (Guide)
	•	Shivani Sharma (Guide)


